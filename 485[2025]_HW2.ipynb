{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS485 - Data Science and Applications\n",
        "\n",
        "  **Homework 2**\n",
        "\n",
        "  Feel free to ignore the already implemented code and use your own."
      ],
      "metadata": {
        "id": "w0ZkWnaXg_ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multiple Linear Regression Using Gradient Descent**\n",
        "The goal of this exercise is to train a linear regression model using **many features simultaneously** and implement **gradient descent** to minimize the Mean Squared Error (MSE).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RL156TehhNyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Load the Data**\n",
        "1. Extract all **feature columns** and the **target column**.\n",
        "2. Print the shape of `X` (features) and `y` (target)."
      ],
      "metadata": {
        "id": "ajVoLxURhcAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jBbapqog8pz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/lpoly/public_data/main/car_data.csv'\n",
        "\n",
        "# Load data from CSV\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Logarithmic Error (MSLE) measures the **log difference** between predicted and actual values.\n",
        "$$\n",
        "\\text{MSLE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log(1 + y_i) - \\log(1 + \\hat{y}_i) \\right)^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ y_i $ is the actual target value.\n",
        "- $ \\hat{y}_i = wX + b $ is the predicted value.\n",
        "- The **log transformation** reduces the impact of large errors for high values.\n"
      ],
      "metadata": {
        "id": "LTH7BkRXj8Ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Define the MSLE Loss Function**\n",
        "\n",
        "1. Implement the **Mean Squared Logarithmic Error (MSLE)** function.This function should take `w` (weights), `b` (bias), `X` (features), and `y` (target)\n",
        "\n",
        "2. Initialize the weights and bias.\n",
        "3. Test the MSLE with the initial values.\n"
      ],
      "metadata": {
        "id": "ikfIllzviqiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MSLE function\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Test the function\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "hEJVPhKFh485"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Analytically derivate the Loss Function\n",
        "1. Derive the gradient formulas for MSLE.\n",
        "2. Implement a Python function `gradient_msle` that computes the **partial derivatives** with respect to `w` and `b`.\n",
        "3. Test the function with initial values.\n"
      ],
      "metadata": {
        "id": "FqxLHd1akZsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Write some Markdown code here to get the derivative.*"
      ],
      "metadata": {
        "id": "BmPNn43GJzVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the gradient of MSLE\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Test the function\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################t)"
      ],
      "metadata": {
        "id": "D_LM4vaRksZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Standardization**\n",
        "1. Compute the **mean** and **standard deviation** of each feature.\n",
        "2. Standardize the feature matrix \\(X\\) using:\n",
        "   $$\n",
        "   X_{\\text{standardized}} = \\frac{X - \\mu}{s}\n",
        "   $$\n",
        "3. Print the **mean** and **standard deviation** after transformation to verify standardization.\n"
      ],
      "metadata": {
        "id": "D5Zv0srvmE1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and standard deviation of each feature\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Standardize the features\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Verify standardization (mean \\approx 0, std \\approx 1)\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "WT8yDxeKmHzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Implement Gradient Descent**\n",
        "1. Implement a `gradient_descent` or a `momentum_gradient_descent` function to **iteratively update weights and bias** using MSLE.\n",
        "2. Run it for **500 iterations** with $\\tau$ = 0.5.\n",
        "3. Store the loss at each step for visualization."
      ],
      "metadata": {
        "id": "SfCNWer6k0QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gradient Descent function\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Perform gradient descent on all features using MSLE\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Display final weights and bias\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "LcKzR7fakxv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Plot the Loss Curve**\n",
        "1. Plot the loss (MSLE) over **iterations**.\n",
        "2. Observe whether the model is converging."
      ],
      "metadata": {
        "id": "_lxMUQ8Xm9S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot loss over iterations\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "vwU9prRumevT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7: Unstandardization of the Model**\n",
        "1. Since we trained on **standardized features**, we must **convert** the model parameters back to the **original scale**.\n",
        "2. The original linear regression equation is:\n",
        "   $$\n",
        "   y = w_{\\text{standardized}} X_{\\text{standardized}} + b_{\\text{standardized}}\n",
        "   $$\n",
        "   We need to convert it back to:\n",
        "   $$\n",
        "   y = w_{\\text{original}} X_{\\text{original}} + b_{\\text{original}}\n",
        "   $$\n",
        "3. Use the following transformations:\n",
        "   - **Weights:**\n",
        "     $$\n",
        "     w_{\\text{original}} = \\frac{w_{\\text{standardized}}}{\\sigma}\n",
        "     $$\n",
        "   - **Bias:**\n",
        "     $$\n",
        "     b_{\\text{original}} = b_{\\text{standardized}} - \\sum \\left( \\frac{w_{\\text{standardized}} \\cdot \\mu}{\\sigma} \\right)\n",
        "     $$\n",
        "4. Print the **converted weights and bias** and compare with the standardized values.\n",
        "5. Discuss: When should we **keep using standardized features** vs. when to **convert back to the original scale**?\n"
      ],
      "metadata": {
        "id": "VWmHeBgen2s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert weights and bias back to original scale\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Display the final model parameters\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "SY0tnXg-nCqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 8: Solve Linear Regression Using the Least Squares Solution**\n",
        "1. Solve the linear regression problem using the **closed-form Least Squares solution**:\n",
        "   $$\n",
        "   (X^T X)w_{\\text{LS}} = X^T y\n",
        "   $$\n",
        "   $$\n",
        "   b_{\\text{LS}} = \\text{mean}(y) - w_{\\text{LS}} \\cdot \\text{mean}(X)\n",
        "   $$\n",
        "2. Compare the weights and bias with those obtained from **gradient descent**.\n",
        "3. Why do you think gradient descent yields different weights ?"
      ],
      "metadata": {
        "id": "DG5g8_eQoN4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve for w using the Least Squares solution\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Extract bias and weights separately\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Display the least squares solution\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Compare with Gradient Descent Results\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "rtVWL2GwoGy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 9: Compare with Built-in Linear Regression**\n",
        "1. Use **scikit-learnâ€™s** built-in [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to fit the model on **standardized features**.\n",
        "2. Extract the learned **weights and bias**.\n",
        "3. Compare the results with:\n",
        "   - **Gradient Descent**\n",
        "   - **Least Squares Solution**"
      ],
      "metadata": {
        "id": "Iphn0w6bsVb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train a linear regression model using scikit-learn\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Extract weights and bias\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Display results\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Compare with Gradient Descent & Least Squares\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "VRhngVwgoV0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 10: Retrain**\n",
        "1. Retrain the model by minimizing MSE instead of the MSLE.\n",
        "2. Run it for **500 iterations** with $\\tau$ = 0.1.\n",
        "3. What do you observe ? Why do you think that happens ?\n"
      ],
      "metadata": {
        "id": "fScg3jE8tqg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function and derivative\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Train the model\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Display final weights and bias\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################"
      ],
      "metadata": {
        "id": "2SkVDZj7vQev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Write an explanation on question 3: What do you observe ? Why do you think that happens ?*"
      ],
      "metadata": {
        "id": "CO7TxFyiKd2n"
      }
    }
  ]
}